{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKkB9BnoWich+0UabQP1lg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaimiMariem/QR_JP_Morgan_Chase/blob/main/Task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Ce texte est au format code\n",
        "```\n",
        "<div>\n",
        "  <h1 style=\"font-size: 24px; font-weight: bold;\">Bucket FICO scores</h1>\n",
        "\n",
        "<p>To construct a rating map that maps FICO scores of borrowers to ratings (where a lower rating signifies a better credit score), we need to perform quantization, which involves dividing the range of FICO scores into specific buckets or intervals. We can explore two approaches for quantization: Mean Squared Error (MSE) and Log-Likelihood.\n",
        "The method we chose in this approach is the log-likelihood method which requires dynamic programming."
      ],
      "metadata": {
        "id": "FJqn7GNSHcwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "raw_url=\"https://raw.githubusercontent.com/NaimiMariem/QR_JP_Morgan_Chase/main/Task%203%20and%204_Loan_Data%20(1).csv\"\n",
        "\n",
        "df = pd.read_csv(raw_url)\n",
        "fico_scores= df['fico_score'].values"
      ],
      "metadata": {
        "id": "1ddO0JXqH9Fa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantization_log_likelihood(fico_scores, num_buckets):\n",
        "    # Sort the FICO scores in ascending order\n",
        "    sorted_fico_scores = np.sort(fico_scores)\n",
        "\n",
        "    # Calculate the number of FICO scores per bucket\n",
        "    scores_per_bucket = len(sorted_fico_scores) // num_buckets\n",
        "\n",
        "    # Initialize the dynamic programming table to store the log-likelihood values\n",
        "    dp_table = np.zeros((num_buckets + 1, len(sorted_fico_scores) + 1))\n",
        "    dp_table[0, :] = 0\n",
        "\n",
        "    # Define a function to compute the log-likelihood for a given bucket\n",
        "    def log_likelihood(boundary_idx, start_idx, end_idx):\n",
        "        num_records = end_idx - start_idx\n",
        "        num_defaults = np.sum(sorted_fico_scores[start_idx:end_idx])\n",
        "        prob_default = num_defaults / num_records\n",
        "        return num_defaults * np.log(prob_default) + (num_records - num_defaults) * np.log(1 - prob_default)\n",
        "\n",
        "    # Perform dynamic programming to fill the dp_table\n",
        "    for i in range(1, num_buckets + 1):\n",
        "        for j in range(i * scores_per_bucket, len(sorted_fico_scores) + 1):\n",
        "\n",
        "            dp_table[i, j] = max(dp_table[i, j - 1], max(dp_table[i - 1, k] + log_likelihood(i, k, j)\n",
        "                                 for k in range((i - 1) * scores_per_bucket, j)))\n",
        "\n",
        "    # Backtrack to find the optimal bucket boundaries that maximize the log-likelihood\n",
        "    bucket_boundaries = []\n",
        "    start_idx = len(sorted_fico_scores)\n",
        "    for i in range(num_buckets, 0, -1):\n",
        "        for k in range(start_idx - 1, (i - 1) * scores_per_bucket - 1, -1):\n",
        "            if dp_table[i, start_idx] == dp_table[i - 1, k] + log_likelihood(i, k, start_idx):\n",
        "                bucket_boundaries.append(sorted_fico_scores[k])\n",
        "                start_idx = k\n",
        "                break\n",
        "\n",
        "    # Create the rating map (bucket boundaries and corresponding values)\n",
        "    rating_map = {}\n",
        "    rating = 1  # Assign ratings in reverse order (higher rating to lower FICO score)\n",
        "    for boundary in reversed(bucket_boundaries):\n",
        "        rating_map[boundary] = rating\n",
        "        rating += 1\n",
        "\n",
        "    return rating_map"
      ],
      "metadata": {
        "id": "WIiqjtZ6IYex"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example\n",
        "num_buckets=4\n",
        "rating_map= quantization_log_likelihood(fico_scores, num_buckets)\n",
        "return(rating_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GkFhPUyKzUR",
        "outputId": "d5de4da5-bcb6-4f91-eed7-0c9ba331584f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-d511d2d71e4f>:17: RuntimeWarning: invalid value encountered in log\n",
            "  return num_defaults * np.log(prob_default) + (num_records - num_defaults) * np.log(1 - prob_default)\n"
          ]
        }
      ]
    }
  ]
}